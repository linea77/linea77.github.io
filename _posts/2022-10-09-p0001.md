---
title: Convolutional Neural Networks for Sentence Classification_Yoon Kim
categories: [papers]
comments: false
---

## **ğŸš© ëª©ì°¨**

---

- [Abstract](#abstract)
- [1. Introduction](#1-introduction)
- [2. Model](#2-model)
  - [2.1. Regularization](#21-regularization)
- [3. Datasets and Experimental Setup](#3-datasets-and-experimental-setup)
  - [3.1. Hyperparameters and Training](#31-hyperparameters-and-training)
  - [3.2. Pre-trained Word Vectors](#32-pre-trained-word-vectors)
  - [3.3. Model Variations](#33-model-variations)
- [4. Results and Discussion](#4-results-and-discussion)
  - [4.1. Multichanel vs Single Channel Models](#41-multichanel-vs-single-channel-models)
  - [4.2. Static vs Non-static Representations](#42-static-vs-non-static-representations)
  - [4.3. Further observation](#43-further-observation)
- [5. Conclusion](#5-conclusion)

# Abstract

- ì‚¬ì „í•™ìŠµëœ ì›Œë“œë²¡í„°(word2vec)ë¡œ í›ˆë ¨í•œ CNNìœ¼ë¡œ ë¬¸ì¥ë‹¨ìœ„ ë¶„ë¥˜ë¥¼ ì‹¤í—˜í•¨.
- ì ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ static vectorë¥¼ ì‚¬ìš©í•œ simple CNNë§Œìœ¼ë¡œë„ ì—¬ëŸ¬ [ë²¤ì¹˜ë§ˆí¬](<https://ko.wikipedia.org/wiki/%EB%B2%A4%EC%B9%98%EB%A7%88%ED%81%AC_(%EC%BB%B4%ED%93%A8%ED%8C%85)>)ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŒ.
- íŒŒì¸íŠœë‹ìœ¼ë¡œ task-specific vectorë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚´
- ì¶”ê°€ì ìœ¼ë¡œ task-specific vectorì™€ static vectorë¥¼ ë‘˜ ë‹¤ ì‚¬ìš©í•˜ì—¬ ì¡°ê¸ˆ ìˆ˜ì •í•œ ëª¨ë¸ êµ¬ì¡° ë˜í•œ ì œì•ˆí•¨.
- ì´ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ CNNëª¨ë¸ì€ ê°ì„±ë¶„ì„, ì§ˆë¬¸ë¶„ë¥˜ë¥¼ í¬í•¨í•œ 7ê°€ì§€ì˜ task ì¤‘ 4ê°œì—ì„œ the state of the art(SOTA)ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ.

# 1. Introduction

**ê¸°ì¡´ì˜ ìì—°ì–´ì²˜ë¦¬ ì—°êµ¬ë“¤**

<img src="https://miro.medium.com/max/828/1*C_gwJa1g_DCi3AehkXYCjQ.png" alt="https://miro.medium.com/max/828/1*C_gwJa1g_DCi3AehkXYCjQ.png" style="zoom:50%;" /> <img src="https://miro.medium.com/max/1400/1*gzXwhDnDmu60OHyEFGnF-Q.png" alt="https://miro.medium.com/max/1400/1*gzXwhDnDmu60OHyEFGnF-Q.png" style="zoom:50%;" />

[ì´ë¯¸ì§€ ì¶œì²˜](https://towardsdatascience.com/visualizing-word-embedding-with-pca-and-t-sne-961a692509f5)

- neural ì–¸ì–´ ëª¨ë¸ì„ í†µí•´ word vector representationì„ í•™ìŠµí•˜ê³ , ë¶„ë¥˜ë¥¼ ìœ„í•´ ê·¸ ì›Œë“œë²¡í„°ë“¤ì„ í•©ì„±í•˜ëŠ” ë°©ì‹ì„ ë§ì´ ì‚¬ìš©í•¨.

- ì´ ë•Œ ì›Œë“œë²¡í„°ëŠ” í¬ì†Œ(sparce)í•œ 1-of-V(VëŠ” ì–´íœ˜ì‚¬ì´ì¦ˆ) ì¸ì½”ë”© ê²°ê³¼ë¥¼, ë” ë‚®ì€ ì°¨ì›ì˜ ì€ë‹‰ì¸µì„ í†µê³¼í•˜ë©´ì„œ ë‹¨ì–´ì— ëŒ€í•œ íŠ¹ì§•ì„ ì¶”ì¶œí•´ë‚´ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•˜ì˜€ìŒ

- ì´ë ‡ê²Œ ì¶”ì¶œëœ dense representationsì—ì„œëŠ” ë¹„ìŠ·í•œ íŠ¹ì§•ì„ ê°€ì§„ ë‹¨ì–´ë¼ë¦¬ëŠ” ë‘ ë‹¨ì–´ ì‚¬ì´ì˜ ê±°ë¦¬ê°€ ê°€ê¹ê³ , ì„œë¡œ ë‹¤ë¥¸ íŠ¹ì§•ì„ ê°€ì§ˆ ìˆ˜ë¡ ê·¸ ê±°ë¦¬ê°€ ë©€ì–´ì§. (ë¶„í¬ê°€ì„¤ê¸°ë°˜)

**ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•´ ê³ ì•ˆëœ CNNì´ NLPì— ì ìš©ë¨**

<img src="https://media2.giphy.com/media/i4NjAwytgIRDW/giphy.gif?cid=790b7611c907ea5c778c883acc2e6c05c9b414f8d112b665&rid=giphy.gif&ct=g" alt="cnn" style="zoom:50%;" />

[ì´ë¯¸ì§€ì¶œì²˜](https://giphy.com/gifs/blog-daniel-keypoints-i4NjAwytgIRDW)

<img src="https://149695847.v2.pressablecdn.com/wp-content/uploads/2018/01/conv-full-layer.gif" alt="[ì´ë¯¸ì§€ ì¶œì²˜](https://analyticsindiamag.com/convolutional-neural-network-image-classification-overview/)" style="zoom:50%;" />

[ì´ë¯¸ì§€ ì¶œì²˜](https://analyticsindiamag.com/convolutional-neural-network-image-classification-overview/)

- í•©ì„±ê³±ì‹ ê²½ë§(CNN)ì€ Computer vision ë¶„ì•¼ì—ì„œ ê³ ì•ˆëœ ê²ƒìœ¼ë¡œ ë ˆì´ì–´ì˜ local featuresì— í•„í„°ë¥¼ ì ìš©í•˜ëŠ” ë°©ì‹ì„.

- ê³§ CNNì´ NLP taskë“¤ì—ë„ ì ìš©ì´ ë˜ì—ˆëŠ”ë°, ì—¬ëŸ¬ taskë“¤ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŒ.

**ë³¸ ë…¼ë¬¸ì˜ ì—°êµ¬ ë‚´ìš©**

- unsupervised neural language modelì—ì„œ ì–»ì€ ì›Œë“œë²¡í„°ì— 1ê°œì˜ convolution layerë¥¼ ê°€ì§„ CNNì„ í•™ìŠµì‹œí‚´.

- ì´ ì›Œë“œë²¡í„°ëŠ” Google Newsì—ì„œ 1000ì–µê°œ ë‹¨ì–´ë¡œ í•™ìŠµëœ ì›Œë“œë²¡í„°ì„.

- ì´ ì›Œë“œë²¡í„° ê°’ì„ ê³ ì •ì‹œí‚¨ ë’¤ ì•„ì£¼ ì ì€ ì–‘ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë§Œ íŠœë‹í•´ì„œ í•™ìŠµì„ ì§„í–‰í•œ ê²°ê³¼, í›Œë¥­í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŒ â‡’ ì‚¬ì „í•™ìŠµëœ ì›Œë“œë²¡í„°ê°€ universal features extractorsì„ì„ í™•ì¸

- ê·¸ ë’¤ì— task-specificí•˜ë„ë¡ íŒŒì¸íŠœë‹í•œ ê²°ê³¼, ì„±ëŠ¥ì´ ë” í–¥ìƒë˜ì—ˆìŒ.

- ê·¸ ë’¤, [ì±„ë„](https://velog.io/@jee-9/Deep-Learning-%EA%B8%B0%EC%B4%88-Channel-Feature-Map-%EA%B0%9C%EC%9A%94)ì„ ì—¬ëŸ¬ê°œë¡œ ë§Œë“¤ì–´ ì‚¬ì „í•™ìŠµëœ ë²¡í„°ì™€ íŒŒì¸íŠœë‹ í›„ task-specific ë²¡í„°ë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ë„ë¡ ëª¨ë¸ì„ ìˆ˜ì •í•¨

- ë˜í•œ image classificationì— ê´€í•˜ì—¬, ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ì„œ ì–»ì€ feature extractorsê°€ original taskì™€ ì°¨ì´ê°€ ìˆëŠ” ë‹¤ë¥¸ taskì—ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŒì„ ì¦ëª…í•˜ì˜€ë˜, Razavian et al.(2014) ì—°êµ¬ì™€ ì² í•™ì ìœ¼ë¡œ ìœ ì‚¬í•¨.

# 2. Model

**ê¸°ë³¸ì ì¸ ëª¨ë¸ êµ¬ì¡°**

- ëª¨ë¸ êµ¬ì¡°ëŠ” Collobert et al.(2011)ì˜ CNN êµ¬ì¡°ë¥¼ ì¡°ê¸ˆ ë³€í˜•í•œ í˜•íƒœì´ë‹¤.

![figure1](\assets\img\post_img\p0001\figure 1.png)

---

![Untitled](\assets\img\post_img\p0001\equation 1.png)

(1) Concatenation operation : 1ë²ˆì§¸ ì›Œë“œ ë²¡í„° ë¶€í„° në²ˆì§¸ ì›Œë“œë²¡í„°ê¹Œì§€ë¥¼ ëŠ˜ì–´ë†“ì€ ê²ƒ

---

![Untitled](\assets\img\post_img\p0001\equation 2.png)

(2) Convolution operation: ië²ˆì§¸ ì›Œë“œë²¡í„°ë¶€í„° i+h-1ë²ˆì§¸ ì›Œë“œë²¡í„°ë¥¼ ëŠ˜ì–´ë†“ì€ ê²ƒì— í¬ê¸°ê°€ h\*kì¸ í•„í„° Wë¥¼ ê³±í•˜ê³  biasë¥¼ ë”í•˜ì—¬ ë§Œë“  ië²ˆì§¸ feature.

![Untitled](\assets\img\post_img\p0001\equation 3.png)

(3) [feature map](https://velog.io/@jee-9/Deep-Learning-%EA%B8%B0%EC%B4%88-Channel-Feature-Map-%EA%B0%9C%EC%9A%94) :ê°ê°ì˜ ê°€ëŠ¥í•œ ëª¨ë“  ë‹¨ì–´ windowì— ëŒ€í•´ (2)ë¥¼ ìˆ˜í–‰í•˜ì—¬ ë§Œë“  feature map.

- ìœˆë„ìš°ì‚¬ì´ì¦ˆ(h)ê°€ ë‹¤ë¥¸ ì—¬ëŸ¬ê°œì˜ í•„í„°ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì—¬ëŸ¬ê°œì˜ featuresë¥¼ ì–»ê²Œ ë¨.

---

![Untitled](\assets\img\post_img\p0001\equation 4.png)

- [max-over-time pooling operation](https://stackoverflow.com/questions/48549670/pooling-vs-pooling-over-time)
- ê°€ì¥ ì¤‘ìš”í•œ featureë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•´ í•˜ë‚˜ì˜ í•„í„°ì— ëŒ€í•´ í•˜ë‚˜ì˜ ìµœëŒ“ê°’ë§Œì„ ì·¨í•¨.
- (ì´ë¯¸ì§€ëŠ” í¬ê¸°ê°€ ê°™ê¸° ë•Œë¬¸ì—) ì´ë¯¸ì§€ì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ê³¼ ë‹¬ë¦¬, max-over-timeì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ì„œë¡œ ë‹¤ë¥¸ ê¸¸ì´ì˜ ë¬¸ì¥ì´ë”ë¼ë„ ì ìš©ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì„.

---

- ì´ ì—¬ëŸ¬ê°œì˜ featuresëŠ” penultimate layer(ë§¨ ë§ˆì§€ë§‰ì—ì„œ ë‘ ë²ˆì§¸ ë ˆì´ì–´)ë¥¼ êµ¬ì„±í•˜ê²Œ ë˜ê³ , fully connected softmax layerë¥¼ í†µê³¼í•˜ì—¬ í™•ë¥ ë¶„í¬ë¥¼ ì¶œë ¥í•˜ê²Œ ë¨.

**Model variant; Multichannel Architecture - ë‘ ê°œì˜ Channel ì‚¬ìš©**

- 1 Channel of stactic word vector + 1 Channel of fine-tuned word vector
  - Static word vector : í›ˆë ¨ ë‚´ë‚´ ì •ì ìœ¼ë¡œ ìœ ì§€ë˜ëŠ”(ê°’ì´ ê³ ì •ë˜ì–´ìˆëŠ”) ì›Œë“œë²¡í„°
  - fine-tuned word vector : ì—­ì „íŒŒ(backpropagation)ë¥¼ í†µí•´ íŒŒì¸íŠœë‹ë˜ëŠ”(ê°’ì´ ì—…ë°ì´íŠ¸ë˜ëŠ”) ì›Œë“œë²¡í„°
- ê°ê°ì˜ í•„í„°ê°€ ë‘ ê°œì˜ ì±„ë„ì— ëª¨ë‘ ì ìš©ë˜ì–´ (2)Convolution operationë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì„œë¡œ ë”í•´ì§.
- ë‚˜ë¨¸ì§€ëŠ” single channel architectureì™€ ë™ì¼í•¨.

## 2.1. Regularization

1. ìˆœì „íŒŒ/ì—­ì „íŒŒ

   ![Untitled](\assets\img\post_img\p0001\equation 5.png)

   (5) Drop out

   - penultimate layer(ë§¨ ë§ˆì§€ë§‰ì—ì„œ ë‘ë²ˆì§¸ ë ˆì´ì–´)ì— dropoutì„ ì ìš©
   - feature mapì— ë“œë¡­ì•„ì›ƒì„ ìˆ˜í–‰í•˜ëŠ” ë§ˆìŠ¤í‚¹ ë²¡í„° rì„ ê³±í•¨. ë§ˆìŠ¤í‚¹ ë˜ì§€ ì•Šì€ ìœ ë‹›ìœ¼ë¡œë§Œ ì—­ì „íŒŒê°€ ì§„í–‰ë¨.
   -

2. Test time

   ![Untitled](\assets\img\post_img\p0001\equation 6.png)

   - í•™ìŠµë˜ì–´ìˆëŠ” ê°€ì¤‘ì¹˜ Wì— pë¥¼ ê³±í•˜ì—¬ ìŠ¤ì¼€ì¼ë§í•œ ê²ƒ( $\hat{w}$ )ì„ unseen sentences(í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹)ì— ì‚¬ìš©
   - ê°€ì¤‘ì¹˜ ë²¡í„°ì— l2-norms constraintë¥¼ ì¤˜ì„œ<img src="C:\Users\User\Desktop\Github\linea77.github.io\assets\img\post_img\p0001\inequality 1.png" alt="Untitled" style="zoom: 50%;" />ì¼ ë•Œ,

     <img src="C:\Users\User\Desktop\Github\linea77.github.io\assets\img\post_img\p0001\equation 7.png" alt="Untitled" style="zoom:50%;" />ê°€ ë˜ë„ë¡ rescailingì„ í•¨.

# 3. Datasets and Experimental Setup

<img src="\assets\img\post_img\p0001\table 1.png" alt="Untitled" style="zoom:50%;" />

**Benchmarks**

- MR :
  - 1ë¬¸ì¥ ì˜í™” ë¦¬ë·°
  - positive / negative
- SST-1
  - Stanford Sentiment Treebank
  - ì˜í™”ë¦¬ë·°ì˜ í™•ì¥ë²„ì „
  - train/dev/test splits ì œê³µë¨
  - very positive / positive / netural / negative / very negative
- SST-2
  - SST-1ê³¼ ê¸°ë³¸ì ìœ¼ë¡œ ê°™ìŒ
  - positive / negative
- Subj
  - Subjectivity(ì£¼ê´€ì„±) ë°ì´í„°ì…‹
  - ë¬¸ì¥ì´ Subjectiveì¸ì§€ Objectiveì¸ì§€ ë¶„ë¥˜í•˜ëŠ” íƒœìŠ¤í¬
- TREC
  - TREC question ë°ì´í„°ì…‹
  - ì§ˆë¬¸ì„ 6ê°œì˜ ì§ˆë¬¸ íƒ€ì…(person, location, numeric information ë“±)ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” íƒœìŠ¤í¬
- CR
  - ìƒí’ˆì— ëŒ€í•œ ê³ ê° ë¦¬ë·°(Customer reviews)
  - Positive / Negative
- MPQA
  - Opinion polarity detection

## 3.1. Hyperparameters and Training

- í™œì„±í™”í•¨ìˆ˜ : rectified liner uitits(ReLU)

- filter window h í¬ê¸° = 3, 4, 5

- feature map ê°œìˆ˜ = 100

- dropout rate (p) = 0.5

- l2 constraint (s) = 3

- mini-batch size = 50

(ìœ„ ê°’ë“¤ì€ SST-2 dev setìœ¼ë¡œ [grid search](https://justweon-dev.tistory.com/21)ë¥¼ í†µí•´ ì„ íƒí•¨)

- early stopping
  - dev set ì‚¬ìš©
  - dev setì´ ì—†ìœ¼ë©´ training dataì˜ 10%ë¥¼ ëœë¤í•˜ê²Œ ì„ íƒí•˜ì—¬ dev setìœ¼ë¡œ ì‚¬ìš©
- stochastic gradient descent(í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•) over shuffled mini-batches
- Adadelta

## 3.2. Pre-trained Word Vectors

- ì›Œë“œë²¡í„°ë¥¼ unsupervised neural language modelì—ì„œ ì–»ì€ ì›Œë“œë²¡í„°ë¡œ ì´ˆê¸°í™”í•˜ëŠ” ê²ƒì€, í° ê·œëª¨ì˜ supervised training setì´ ì—†ì„ ë•Œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë„ë¡ ë§ì´ ì“°ëŠ” ë°©ë²•ì„.
- Google NEwsì˜ 1000ì–µê°œ ì–´íœ˜ë¡œ í•™ìŠµëœ word2vecì„ ì‚¬ìš©í•¨.
- ì›Œë“œë²¡í„° dimensionality = 300
- Continuous bag-of-words(CBOW)ì‚¬ìš©
- ì‚¬ì „ í•™ìŠµëœ ì–´íœ˜ ì§‘í•©ì— ì—†ëŠ” ë‹¨ì–´ëŠ” ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”í•¨(randomly initialized)

## 3.3. Model Variations

- CNN-rand
  - ëª¨ë“  ì–´íœ˜ì— ëŒ€í•´ ëœë¤í•˜ê²Œ ì´ˆê¸°í™”í•˜ê³ , í›ˆë ¨ì„ í†µí•´ ìˆ˜ì •í•´ë‚˜ê°.(ì‚¬ì „í•™ìŠµëœ ì›Œë“œë²¡í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
- CNN-static
  - ì‚¬ì „í•™ìŠµëœ word2vec ì›Œë“œë²¡í„°ì™€, ëœë¤í•˜ê²Œ ì´ˆê¸°í™”í•œ unknown words ëª¨ë‘ ì²˜ìŒ ìƒíƒœ ê·¸ëŒ€ë¡œ ê³ ì •(static)ë˜ì–´ìˆê³ , ë‹¤ë¥¸ íŒŒë¼ë¯¸í„°ë“¤ë§Œ í•™ìŠµë¨.
- CNN-non-static
  - ì‚¬ì „í•™ìŠµëœ word2vec ì›Œë“œë²¡í„°ì™€, ëœë¤í•˜ê²Œ ì´ˆê¸°í™”í•œ unknown words ì›Œë“œë²¡í„°ê°€ ê°ê°ì˜ taskì— ì˜í•´ íŒŒì¸íŠœë‹ë¨.(ì›Œë“œë²¡í„°ê°€ ì—…ë°ì´íŠ¸ ë¨)
- CNN-multichannel
  - ë‘ ì¢…ë¥˜ì˜ ì›Œë“œë²¡í„°(static & non-static)ì„ ì‚¬ìš©í•¨.
  - ê°ê°ì˜ ì§‘í•©ì€ channelì´ ë˜ê³ , ê°ê°ì˜ í•„í„°ê°€ ë‘ ê°œì˜ ì±„ë„ ëª¨ë‘ì— ì ìš©ë¨
  - ê·¸ëŸ¬ë‚˜ non-static channel ì—ë§Œ ì—­ì „íŒŒê°€ í—ˆìš©ë¨

# 4. Results and Discussion

<img src="\assets\img\post_img\p0001\table 2.png" alt="Untitled" style="zoom: 67%;" />

- CNN-rand VS ì‚¬ì „í•™ìŠµì„ ì‚¬ìš©í•œ ëª¨ë¸ë“¤
  - CNN-randëŠ” ì„±ëŠ¥ì´ ì˜ ë‚˜ì˜¤ì§€ ì•ŠëŠ” ë°˜ë©´, ì‚¬ì „í•™ìŠµëœ ì›Œë“œë²¡í„°ë¥¼ ì‚¬ìš©í•œ ë‹¤ë¥¸ ëª¨ë¸ì€ í›¨ì”¬ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŒ
- CNN-static VS ë³µì¡í•œ ê¸°ë²•ì„ ì‚¬ìš©í•œ ëª¨ë¸ë“¤
  - CNN-staticì€ ê°„ë‹¨íˆ ì‚¬ì „í•™ìŠµëœ ì›Œë“œë²¡í„°ë¥¼ ì‚¬ìš©í–ˆì„ ë¿ì¸ë°ë„, ë³µì¡í•œ pooling ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸(DCNN), ì‚¬ì „ì˜ íŒŒìŠ¤íŠ¸ë¦¬ë¥¼ ê³„ì‚°í•´ì•¼ í•˜ëŠ” ëª¨ë¸(RNTN)ê³¼ ë¹„ë“±ë¹„ë“±í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ
- CNN-non-static
  - fine-tuningí•˜ëŠ” ê²ƒë„ ë” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚´

## 4.1. Multichanel vs Single Channel Models

- ì²˜ìŒì— ë©€í‹°ì±„ë„êµ¬ì¡°ê°€ ì˜¤ë²„í”¼íŒ…(overfitting)ì„ ë§‰ì„ ìˆ˜ ìˆì„ ê²ƒì´ë¼ê³  ê¸°ëŒ€í–ˆê³ , ë”°ë¼ì„œ ë°ì´í„°ì…‹ì´ ì‘ì•„ì§€ë”ë¼ë„ ì‹±ê¸€ì±„ë„ëª¨ë¸ë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤„ ê²ƒì´ë¼ê³  ìƒê°í–ˆìŒ
- ê·¸ëŸ¬ë‚˜ ëšœë ·í•˜ê²Œ ë©€í‹°ì±„ë„êµ¬ì¡°ê°€ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì§„ ì•Šì•˜ìœ¼ë©°, ì„±ëŠ¥ì— ëŒ€í•œ ê²°ê³¼ê°€ ì„ì—¬ì„œ ë‚˜ì™”ë‹¤.
- ë”°ë¼ì„œ fine-tuning ê³¼ì •ì˜ ì¼ë°˜í™”ì— ëŒ€í•œ ì¶”ê°€ì—°êµ¬ê°€ í•„ìš”í•˜ë‹¤.
  - ì˜ˆ) ì±„ë„ì„ 1ê°œë¡œ ìœ ì§€í•˜ë˜, í•™ìŠµí•˜ëŠ” ë™ì•ˆ ìˆ˜ì •ë  ìˆ˜ ìˆëŠ” ì¶”ê°€ì ì¸ ì°¨ì›ì„ ì‚¬ìš©

## 4.2. Static vs Non-static Representations

![Untitled](\assets\img\post_img\p0001\table 3.png)

- Non-satic ì±„ë„ì´ ë‹¤ë£¨ê³  ìˆëŠ” taskì— ëŒ€í•´ ë” êµ¬ì²´ì ìœ¼ë¡œ ì›Œë“œë²¡í„°í‘œìƒì„ ë§Œë“¤ì–´ë‚´ì—ˆìŒ
  - ê¸°ì¡´ word2Vecì—ì„œëŠ” í†µì‚¬ì ì¸ ìœ ì‚¬ì„±(ë¹„ìŠ·í•œ í™˜ê²½ì—ì„œ ì‚¬ìš©ë¨)ì— ì§‘ì¤‘í•˜ì—¬ badì™€ goodì„ ìœ ì‚¬í•œ ë‹¨ì–´ë¡œ í‘œìƒí•˜ì˜€ìŒ
  - ê·¸ëŸ¬ë‚˜ positive/negativeì˜ ë¼ë²¨ì„ ê°–ëŠ” SST-2ì˜ ê°ì •ë¶„ì„ taskë¡œ íŒŒì¸íŠœë‹í•œ ê²°ê³¼ badì™€ goodì€ ë”ì´ìƒ ìœ ì‚¬í•œ ë‹¨ì–´ë¡œ í‘œìƒë˜ì§€ ì•ŠìŒ.
- ì‚¬ì „í•™ìŠµëœ ì›Œë“œë²¡í„°ì— ë‚˜íƒ€ë‚˜ì§€ ì•Šì•„ì„œ ëœë¤í•˜ê²Œ ì´ˆê¸°í™”í•œ ê²½ìš°,
  - íŒŒì¸íŠœë‹ì„ í†µí•´ ë” ì˜ë¯¸ìˆëŠ” í‘œìƒì„ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆì—ˆìŒ.
  - ì˜ˆ) ì½¤ë§ˆ(,)ê°€ ì ‘ì†ì‚¬ì™€ ìœ ì‚¬í•¨ì„ ì¡ì•„ëƒ„.

## 4.3. Further observation

- Kalchbrenner et al. (2014)ì—ì„œ ë™ì¼í•œ êµ¬ì¡°ì˜ CNNì„ ì‚¬ìš©í–ˆìœ¼ë‚˜ í›¨ì”¬ ì„±ëŠ¥ì´ ì•ˆì¢‹ì•˜ìŒ
  - í•„í„°ì˜ ì‚¬ì´ì¦ˆë¥¼ ì—¬ëŸ¬ê°œë¡œ ì‚¬ìš©í•˜ê³ , ì´ì— ë”°ë¼ feature mapë„ ì—¬ëŸ¬ê°œë¡œ ë§Œë“¤ì—ˆê¸° ë•Œë¬¸ì— ì°¨ì´ê°€ ìƒê¹€
- Dropoutì´ ì¢‹ì€ ì •ê·œí™” ë„êµ¬ì„ì´ ì¦ëª…ë˜ì—ˆìŒ. ì§€ì†ì ìœ¼ë¡œ 2%~4%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì¤Œ
- word2vecì— ì—†ëŠ” ë‹¨ì–´ë¥¼ ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”í•  ë•Œ, word2vec ë²¡í„°ì™€ ë™ì¼í•œ ë¶„ì‚°ì„ ê°–ë„ë¡ aë¥¼ ì„ íƒí•˜ê³  ìƒ˜í”Œë§ì„ í•´ ì•½ê°„ì˜ ê°œì„ ì„ ì–»ì—ˆìŒ.
- ìœ„í‚¤í”¼ë””ì•„ë¡œ í•™ìŠµëœ ë‹¤ë¥¸ ì›Œë“œë²¡í„°ë„ ì‚¬ìš©í•´ ë³´ì•˜ìœ¼ë‚˜ word2vecì´ í›¨ì”¬ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ
- AdadeltaëŠ” Adagradì™€ ë¹„ìŠ·í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆìœ¼ë‚˜ epochìˆ˜ê°€ ë” ì ê²Œ í•„ìš”í–ˆìŒ.

# 5. Conclusion

- í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ê±°ì˜ íŠœë‹í•˜ì§€ ì•Šì•˜ìŒì—ë„, í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ ê°€ì§„ simple CNNì€ ê½¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.
- ë˜í•œ ì‚¬ì „í•™ìŠµëœ ì›Œë“œë²¡í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ìì—°ì–´ì²˜ë¦¬ ë”¥ëŸ¬ë‹ì—ì„œ ì¤‘ìš”í•¨ì„ ë³´ì—¬ì¤€ë‹¤.
